---
title: \vspace{2in} Estimating Study Heterogeneity 
subtitle: "PHP 2550: Final Project"
author: "Blain Morin"
date: "December 19, 2018"
indent: true
output: 
  pdf_document:
    toc: true
header-includes:
- \usepackage{float}
- \usepackage{indentfirst}
---


```{r, echo = FALSE, message = FALSE, warning = FALSE}

### Set knitr options
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE)

### Load Required Libraries

library(readr)
library(dplyr)
library(ggplot2)
library(extrafont)
library(stargazer)
library(tidyr)
library(metafor)
library(kableExtra)

### set seed
set.seed(100)

### Load Data
full = read_delim("2010_6.csv", "\t", 
                      escape_double = FALSE, trim_ws = TRUE)

```

```{r}

### Exclude studies with .5s
full = full %>%
  filter(a > 1,
         b > 1,
         c > 1,
         d > 1) %>%
  group_by(my_ID) %>%
  mutate(correct_no_of_studies = n()) %>%
  ungroup() %>%
  filter(correct_no_of_studies >= 5) 


### Get only largest meta (highest number of studies)
full = full %>%
  group_by(number_review) %>%
  filter(correct_no_of_studies == max(correct_no_of_studies)) %>%
  mutate(n = a + b + c + d) %>%
  group_by(my_ID) %>%
  mutate(meta.n = sum(n)) %>%
  ungroup() %>%
  group_by(number_review) %>%
  filter(meta.n == max(meta.n)) %>%
  ungroup()


```

```{r}

### Add study effect size and variance
### Add DesSimonian-Laird Estimate for tau2 and se.tau2
full = full %>% 
  mutate(yi = escalc(measure = "OR",
                     ai = a,
                     bi = b,
                     ci = c, 
                     di = d)$yi,
         vi = escalc(measure = "OR",
                     ai = a,
                     bi = b,
                     ci = c, 
                     di = d)$vi) %>%
  group_by(my_ID) %>%
  mutate(DL.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "DL")$tau2,
         DL.se.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "DL")$se.tau2,
         DL.summary = rma.uni(yi = yi,
                           vi = vi,
                           method = "DL")$beta) %>%
  ungroup()
  


```

```{r}

### Add Paule and Mandel 
full = full %>%
  group_by(my_ID) %>%
  mutate(PM.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "PM")$tau2,
         PM.se.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "PM")$se.tau2,
         PM.summary = rma.uni(yi = yi,
                           vi = vi,
                           method = "PM")$beta) %>%
  ungroup()
  

```

```{r}

### Add Hedges 
full = full %>%
  group_by(my_ID) %>%
  mutate(HE.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "HE")$tau2,
         HE.se.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "HE")$se.tau2,
         HE.summary = rma.uni(yi = yi,
                           vi = vi,
                           method = "HE")$beta) %>%
  ungroup()

```

```{r}

### Add Hunter-Schmidt
full = full %>%
  group_by(my_ID) %>%
  mutate(HS.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "HS")$tau2,
         HS.se.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "HS")$se.tau2,
         HS.summary = rma.uni(yi = yi,
                           vi = vi,
                           method = "HS")$beta) %>%
  ungroup()

```

```{r}

### Add Maximum Likelihood
full = full %>%
  group_by(my_ID) %>%
  mutate(ML.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "ML")$tau2,
         ML.se.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "ML")$se.tau2,
         ML.summary = rma.uni(yi = yi,
                           vi = vi,
                           method = "ML")$beta) %>%
  ungroup()

```

```{r}

### Add REML
full = full %>%
  group_by(my_ID) %>%
  mutate(REML.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "REML", control=list(maxiter=1000))$tau2,
         REML.se.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "REML", control=list(maxiter=1000))$se.tau2,
         REML.summary = rma.uni(yi = yi,
                           vi = vi,
                           method = "REML", control=list(maxiter=1000))$beta) %>%
  ungroup()

```

```{r}

### Add Sidik-Jonkman
full = full %>%
  group_by(my_ID) %>%
  mutate(SJ.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "SJ")$tau2,
         SJ.se.tau2 = rma.uni(yi = yi,
                           vi = vi,
                           method = "SJ")$se.tau2,
         SJ.summary = rma.uni(yi = yi,
                           vi = vi,
                           method = "SJ")$beta) %>%
  ungroup()


```


```{r}

### Set plot colors
cols <- c("DerSimonian-Laird" = "green",
          "Hedges" = "yellow",
          "Hunter-Schmidt" = "purple",
          "Sidik-Jonkman" = "blue",
          "Maximum-Likelihood" = "orange",
          "REML" = "black",
          "Paule-Mandel" = "red")


```


```{r}

### 60 Random study tau estimates plot
set.seed(100)

temp = full %>%
  group_by(number_review) %>%
  slice(1) %>%
  ungroup() %>%
  sample_n(size = 60)

tau.estimates.plot = temp %>%
  ggplot(aes(x = review)) +
  geom_point(aes(y = PM.tau2, color = "Paule-Mandel"), shape = 15, size = 3) +
  geom_point(aes(y = DL.tau2, color = "DerSimonian-Laird"), shape = 15, size = 3) +
  geom_point(aes(y = SJ.tau2, color = "Sidik-Jonkman"), shape = 15, size = 3) +
  geom_point(aes(y = HE.tau2, color = "Hedges"), shape = 15, size = 3) +
  geom_point(aes(y = HS.tau2, color = "Hunter-Schmidt"), shape = 15, size = 3) +
  geom_point(aes(y = ML.tau2, color = "Maximum-Likelihood"), shape = 15, size = 3) +
  geom_point(aes(y = REML.tau2, color = "REML"), shape = 15, size = 3) +
  scale_colour_manual(name = "Method:", values = cols) +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5)) +
  ylab("Tau Squared") +
  xlab("Cochrane Review") +
  ggtitle("Tau Squared Estimates for 60 Random Analyses") +
  theme(text=element_text(size=12,  family="CM Sans"))
  

```

```{r}

### 60 Random study summary estimates plot

summary.estimates.plot = temp %>%
  ggplot(aes(x = review)) +
  geom_point(aes(y = PM.summary, color = "Paule-Mandel"), shape = 15, size = 3, alpha = .6) +
  geom_point(aes(y = DL.summary, color = "DerSimonian-Laird"), shape = 15, size = 3, alpha = .6) +
  geom_point(aes(y = SJ.summary, color = "Sidik-Jonkman"), shape = 15, size = 3, alpha = .6) +
  geom_point(aes(y = HE.summary, color = "Hedges"), shape = 15, size = 3, alpha = .6) +
  geom_point(aes(y = HS.summary, color = "Hunter-Schmidt"), shape = 15, size = 3, alpha = .6) +
  geom_point(aes(y = ML.summary, color = "Maximum-Likelihood"), shape = 15, size = 3, alpha = .6) +
  geom_point(aes(y = REML.summary, color = "REML"), shape = 15, size = 3, alpha = .6) +
  scale_colour_manual(name = "Method:", values = cols) +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5)) +
  ylab("log(Odds Ratio)") +
  xlab("Cochrane Review") +
  theme(text=element_text(size=12,  family="CM Sans"))


```


```{r}

### Put in long format
full.long = full %>%
  gather(key = "model", value = "summary", DL.summary, PM.summary,
         HE.summary, HS.summary, ML.summary, REML.summary, SJ.summary)

### Slice 1 summary for each model
full.long = full.long %>%
  group_by(my_ID, model) %>%
  slice(1) %>%
  ungroup()

### Calculate biggest difference
full.long = full.long %>%
  group_by(my_ID) %>%
  mutate(abs.sum = abs(summary)) %>%
  mutate(diff = max(abs.sum) - min(abs.sum)) %>%
  mutate(p.diff = diff/max(abs.sum)) %>%
  ungroup()

### Proportionate Difference Plot
p.diff.plot = full.long %>%
  group_by(my_ID) %>%
  slice(1) %>%
  ggplot(aes(x = p.diff)) +
  geom_histogram(binwidth = .1, fill = "white", color = "black") +
  theme_light() +
  scale_x_continuous(breaks = seq(0, 1, .1)) +
  xlab("Proportionate Difference") +
  ylab("Count") +
  theme(text=element_text(size=12,  family="CM Sans"))

```

```{r}

sums = full %>%
  group_by(my_ID) %>%
  slice(1) %>%
  ungroup()

sums1 = sums %>%
  select(DL.tau2, HE.tau2,
         HS.tau2, SJ.tau2,
         ML.tau2, REML.tau2,
         PM.tau2) %>%
  summarise(meanDL = mean(DL.tau2),
            meanHE = mean(HE.tau2),
            meanHS = mean(HS.tau2),
            meanSJ = mean(SJ.tau2),
            meanML = mean(ML.tau2),
            meanREML = mean(REML.tau2),
            meanPM = mean(PM.tau2))

sums2 = sums %>%
  select(DL.tau2, HE.tau2,
         HS.tau2, SJ.tau2,
         ML.tau2, REML.tau2,
         PM.tau2) %>%
  summarise(sdDL = sd(DL.tau2),
            sdHE = sd(HE.tau2),
            sdHS = sd(HS.tau2),
            sdSJ = sd(SJ.tau2),
            sdML = sd(ML.tau2),
            sdREML = sd(REML.tau2),
            sdPM = sd(PM.tau2))

sums3 = sums %>%
  select(DL.tau2, HE.tau2,
         HS.tau2, SJ.tau2,
         ML.tau2, REML.tau2,
         PM.tau2) %>%
  summarise(maxDL = max(DL.tau2),
            maxHE = max(HE.tau2),
            maxHS = max(HS.tau2),
            maxSJ = max(SJ.tau2),
            maxML = max(ML.tau2),
            maxREML = max(REML.tau2),
            maxPM = max(PM.tau2))

sums4 = cbind(t(sums1), t(sums2), t(sums3))

row.names(sums4) = c("DerSimonian-Laird",
                     "Hedges",
                     "Hunter-Schmidt",
                     "Sidik-Jonkman",
                     "Maximum-Likelihood",
                     "REML",
                     "Paule-Mandel")

sums4 = as.data.frame(sums4)

sums4 = sums4 %>%
  rename(Mean.Tau2 = V1, Sd.Tau2 = V2, Max.Tau2 = V3)

```

```{r}

### Tau squared densities

tau.square.density.plot = full %>%
  group_by(my_ID) %>%
  mutate(DL.tau2 = DL.tau2 + .0001,
         HE.tau2 = HE.tau2 + .0001,
         HS.tau2 = HS.tau2 + .0001,
         SJ.tau2 = SJ.tau2 + .0001,
         ML.tau2 = ML.tau2 + .0001,
         REML.tau2 = REML.tau2 + .0001,
         PM.tau2 = PM.tau2 + .0001) %>%
  slice(1) %>%
  ungroup %>%
  ggplot() +
  geom_line(aes(x = log(DL.tau2), color = "DerSimonian-Laird"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(HE.tau2), color = "Hedges"), size = 2, stat = "density", alpha = .6) +
  geom_line(aes(x = log(HS.tau2), color = "Hunter-Schmidt"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(SJ.tau2), color = "Sidik-Jonkman"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(ML.tau2), color = "Maximum-Likelihood"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(REML.tau2), color = "REML"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(PM.tau2), color = "Paule-Mandel"), stat = "density", size = 2, alpha = .6) +
  theme_light() +
  scale_color_manual(name = "Method:", values = cols) +
  ylab("Density") +
  xlab("log(Tau Squared)") 
  
```

```{r}

tau.se.density.plot = full %>%
  group_by(my_ID) %>%
  mutate(DL.tau2 = DL.tau2 + .0001,
         HE.tau2 = HE.tau2 + .0001,
         HS.tau2 = HS.tau2 + .0001,
         SJ.tau2 = SJ.tau2 + .0001,
         ML.tau2 = ML.tau2 + .0001,
         REML.tau2 = REML.tau2 + .0001,
         PM.tau2 = PM.tau2 + .0001) %>%
  slice(1) %>%
  ungroup %>%
  ggplot() +
  geom_line(aes(x = log(DL.se.tau2), color = "DerSimonian-Laird"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(HE.se.tau2), color = "Hedges"), size = 2, stat = "density", alpha = .6) +
  geom_line(aes(x = log(HS.se.tau2), color = "Hunter-Schmidt"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(SJ.se.tau2), color = "Sidik-Jonkman"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(ML.se.tau2), color = "Maximum-Likelihood"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(REML.se.tau2), color = "REML"), stat = "density", size = 2, alpha = .6) +
  geom_line(aes(x = log(PM.se.tau2), color = "Paule-Mandel"), stat = "density", size = 2, alpha = .6) +
  theme_light() +
  scale_color_manual(name = "Method:", values = cols) +
  ylab("Density") +
  xlab("log(Tau Squared SE)") 
  
  
  
  
```

```{r}

# ### Forest plot for highest tau
# high.tau.forest = full %>%
#   filter(my_ID == 39123)
# 
# high.tau.forest2 = rma.uni(measure = "OR", ai = high.tau.forest$a,
#                            bi = high.tau.forest$b,
#                            ci = high.tau.forest$c, 
#                            di = high.tau.forest$d, 
#                            slab = high.tau.forest$study)
# 
# forest(high.tau.forest2, ilab = high.tau.forest$n, ilab.xpos = -6)
# text(c(-6), 12, c("n"))
# title("Forest Plot for CD005089")

```

```{r}

### Number of studies histogram
n.studies.hist = full %>%
  group_by(number_review) %>%
  slice(1) %>%
  ungroup() %>%
  filter(correct_no_of_studies < 75) %>%
  ggplot(aes(x = correct_no_of_studies)) +
  geom_histogram(binwidth = 5, color = "black", fill = "white") +
  theme_light() +
  scale_color_manual(name = "Method:", values = cols) +
  ylab("Count") +
  xlab("Number of Studies in the Meta-Analysis") +
  ggtitle("Histogram: Number of Studies in Largest Meta-Analysis") +
  theme(text=element_text(size=12,  family="CM Sans")) +
  scale_x_continuous(breaks = seq(0, 75, 5))


```


```{r}

### Sample size density plot

meta.n.density.plot = full %>%
  group_by(number_review) %>%
  slice(1) %>%
  ungroup() %>%
  filter(meta.n < 50000) %>%
  ggplot(aes(x = meta.n)) +
  geom_density(color = "black", fill = "white") +
  theme_light() +
  scale_color_manual(name = "Method:", values = cols) +
  ylab("Count") +
  xlab("Meta Sample Size")  +
  ggtitle("Density Plot: Meta Sample Size") +
  theme(text=element_text(size=12,  family="CM Sans"))


```

\newpage

# Introduction

  Meta-analysis is the statistical method for analyzing data that comes from a synthesis of studies. It is often a key component of a systematic review. The aim is to summarize all of the available evidence in response to a research question. After identifying and screening all of the relevant studies, their data are extracted and meta-analytic techniques are used to quantitatively assess the evidence. Such analyses guide new research and allow their users to make fully informed and scientifically rigorous decisions.
  
&nbsp;
  
  Here is a simple example of an evidence synthesis:
  
```{r, fig.height=4, fig.width=6}

### Random Forest Plot

random.meta = full %>%
  filter(my_ID == 6013)

random.meta2 = rma.uni(measure = "OR", ai = random.meta$a,
                           bi = random.meta$b,
                           ci = random.meta$c,
                           di = random.meta$d,
                           slab = random.meta$study,
                       method = "FE")
par(family = 'CM Sans')
forest(random.meta2, ilab = random.meta$n, ilab.xpos = -6)
text(c(-6), 10.5, c("n"))
title("Forest Plot: CD000561")

```
  
  In the forest plot above, the research question is: Do tricyclic antidepressants help elderly people who are depressed? The authors of the review identified 9 studies that examine tricyclic antidepressants. The forest plot displays the name, sample size, and study-level effect measures. For this review, the study level estimates are log odds ratios of being depressed. Log odds ratios are converted to odds ratios through exponentiating the estimate. For example, the Georgotas 1986 reports a log odds ratio of -2.53. exp(-2.53) = .08. In other words, the odds of staying depressed for people who were taking tricyclic antidepressants were  .08 times the odds of people who were taking the placebo (92% reduction in the odds of being depressed).
  
&nbsp;
  
  An odds ratio of .08 strongly favors the tricyclic antidepressant treatment. However, it is clear in the forest plot that this is the most extreme treatment effect estimate. The Georgotas study's sample size is only 52. It may be because of random chance that the treatment is so effective. In the forest plot above, 6 of the 9 studies' confidence intervals cross the zero line (which represents an odds ratio of 1). Because the zero line represents the null hypothesis, these 6 studies are thus not statistically significant at the .05 level. One of the takeaway messages from examining the forest plot is that simply looking at statistical significance of individual studies is not sufficient for summarizing evidence. In this case, all of the studies favor the treatment. If the individual studies are under powered, a clinically important effect may be missed if the relevant data is not aggregated.
  
&nbsp;
  
  The diamond at the bottom of the forest plot represents the meta-analysis estimate and confidence interval. Here, the estimate comes from a fixed effect model. In this example, the meta-analysis estimate is -1.15, which corresponds to an odds ratio of exp(-1.15) = .316 (the treatment is associated with a 68% reduction in the odds of being depressed). The meta-analysis treatment effect estimate is statistically significant. The confidence interval of the estimate (the width of the diamond in the forest plot) is smaller than any of the individual studies. In other words, the aggregate estimate is more precise. 
  
&nbsp;

  The meta-analysis estimate is a weighted mean of the individual study estimates. In the forest plot above, the relative weight given to each individual study in the calculation of the aggregate estimate is represented by the size of the square. How are these weights calculated?
  
# Fixed Effect vs Random Effects 

## Fixed Effects Models

  The weighting schemes come in two flavors: fixed effect models and random effects models. The fixed effect model assumes that all of the studies share a common effect. For example, suppose a college wants to know the average math aptitude score of its students. Due to budget, time or space constraints, the school cannot expect to administer the math test to the entire college. Instead, they administer the test to five random samples of students. Each of the testing sessions is considered to be a "study."
  
&nbsp;

  Assuming each of the testing session were similar, it makes sense to think that each of the “studies” is a random sample from a common population (all the students at the college). In this case, the weighted average is a relatively straightforward calculation: the studies with more precision (lower variance) get higher weight. Since sample size is the primary driver for precision, studies with higher sample sizes get more weight. Here is the idea expressed mathematically:
  
$$ \theta_{FE} = \dfrac{\sum{w_{iFE}}*y_i}{\sum{w_{iFE}}}, w_{iFE} = 1 / v_i   $$

The fixed effect meta-analysis estimate of the effect size, $\theta_{FE}$, is equal to the weighted mean of the individual study estimates, $y_i$. The weights for the fixed effect model are $1/v_i$, where $v_i$ is the variance of the individual study estimate. As the variance of a study estimate increases, the weight it it given in the summary effect calculation decreases. 

&nbsp;

Because all of the studies in the fixed effect model share a common effect, the only source of variation is sampling error. If each of the individual studies were to increase its sample size, they would all approach the same $\theta_{FE}$.

## Random Effects Models

The weighting scheme for the random effects model does not assume that each study in the review shares a common effect. Though studies may be conceptually similar, their characteristics are likely not identical. Returning to the math test example, suppose the reviewer was instead interested in the math aptitude score of all college students. After searching the literature, they find the results from three different schools which administered the test to a random sample of their students. In this case, the obvious difference between the studies is that they are sampling from different populations of students. Because the true mean math aptitude score is different between different schools, each study is thus estimating a different true mean. 

&nbsp;

Because the studies in a random effect model do not share a common effect, each study estimate approaches a different $\theta$ as their sample size increase. Thus, the idea of the random effects model is not to estimate a single $\theta$ but instead to estimate the distribution of true $\theta$s. There are two sources of variation in the random effects model: sampling error and between study heterogeneity. Here is how between study heterogeneity factors into the weighting scheme:

$$ \mu_{RE} = \dfrac{\sum{w_{iRE}}*y_i}{\sum{w_{iRE}}}, w_{iRE} = 1 / (v_i +  \tau^2)  $$
  
Instead of estimating a single $\theta$, the random effects model estimates a mean ($\mu_{RE}$) of $\theta$s. $\mu_{RE}$ is still a weighted average of the individual study estimates like in the fixed effect model, but here the weights include the extra between study variance, $\tau^2$.

&nbsp;

In systematic reviews, there is usually no obvious reason to assume that each study pulled from the literature shares a common treatment effect. Therefore, the random effects model is usually the best choice. 

# Estimating $\tau^2$

A variety of methods have been proposed for estimating between study hetergeneity. In this paper, only methods that have been implemented in R's *metafor* package are examined. The seven methods that are empirically compared in this paper are: DerSimonian and Laird, Paule and Mandel, Hedges, Hunter and Schmidt, Maximum Likelihood, Restricted Maximum Likelihood, and Sidik and Jonkman. 

* DerSimonian and Laird (1986):

$$ \hat{\tau}^2_{DL} =  max\left(0,\frac{Q-df}{\sum w_{iFE} - \frac{\sum w^2_iFE}{\sum w_iFE} } \right) $$ 

* Paule and Mandel (1982):

$$  Q_{gen} = \sum w_{iRE} (y_i - \hat{\mu}_{RE}(\tau^2)) ^2 $$

* Hedges (1985):

$$ \hat{\tau}^2_H = max \left( 0, \frac{1}{df} \sum (y_i - \bar{y})^2 - \frac{1}{df+1} \sum v_i   \right)   $$


* Hunter and Schmidt (2004):

$$  \hat{\tau}^2_{HS} = max \left(  0, \frac{Q-(df+1)}{\sum w_{iFE}} \right) $$

* Maximum Likelihood

$$\hat{\tau}^2_{ML} = max \left( 0 , \frac{\sum w^2_{iRE}((y_i - \hat{\mu}_{RE}(\hat{\tau}^2_{ML}))^2 - v_i)}{\sum w^2_{iRE}}  \right)$$

* Restricted Maximum Likelihood

$$\hat{\tau}^2_{REML} = max \left( 0 , \frac{\sum w^2_{iRE}((y_i - \hat{\mu}_{RE}(\hat{\tau}^2_{REML}))^2 - v_i)}{\sum w^2_{iRE}} + \frac{1}{\sum w_{iRE}}  \right)$$

* Sidik and Jonkman (2005):

$$ \hat{\tau}^2_{SJ} = \frac{1}{k-1} \sum \hat{q}^{-1}_i (y_i - \hat{\mu}_{\hat{q}RE})   $$

where

$$  \hat {q}_i = \hat{r}_i + 1, \hat{r}_i = v_i / \hat{\tau}^2_0 $$



```{r}

model.names = c("DerSimonian and Laird",
                "Paule and Mandel",
                "Hedges",
                "Hunter and Schmidt",
                "Maximum Likelihood",
                "Restricted Maximum Likelihood",
                "Sidik and Jonkman")

model.des = c("MOM, Compares the observed Cochrane Q statistic to the expected Q statistic",
              "Iterative GMM, with Q weights = 1/(v + tau^2)",
              "MOM, Compares observed sample variance to the expected sample variance",
              "",
              "",
                "Restricted Maximum LikelihoodDerSimonian and LairdDerSimonian and LairdDerSimonian and LairdDerSimonian and LairdDerSimonian and LairdDerSimonian and LairdDerSimonian and Laird",
                "Sidik and JonkmanDerSimonian and LairdDerSimonian and LairdDerSimonian and LairdDerSimonian and LairdDerSimonian and LairdDerSimonian and Laird")


tau.model.sums = data.frame(Method = model.names, 
                            Description = model.des
                            )

kable(tau.model.sums, format = "latex") %>%
  column_spec(1, "1in") %>%
  column_spec(2, "2in")

```


  
  
  

# Empirical Analysis

## Data and Data Cleaning

The raw data for this analysis was extracted from The Cochrane Database of Systematic Reviews (CDSR). The CDSR contains peer-reviewed and high quality healthcare systematic reviews conducted by Cochrane Review groups. The data contain all reviews with binary outcomes from January 2003 to July 2010, a total of 3,082 systematic reviews. 

&nbsp;

Since tests for heterogeneity lack power, meta-analyses with less than 5 studies were omitted. Only 1 meta-analysis per review is used to estimate heterogeneity in order to avoid concerns about the correlation in heterogeneity estimates from each meta-analysis within a review. The meta-analysis with the highest number of studies is selected per each review. In the event of a tie, the meta-analysis with the highest total sample size is selected. If there is still a tie, the meta-analysis is selected at random. Studies that had rare events (defined as only one occurrence) are excluded because inverse-variance methods are biased for rarer outcomes. Also, studies with 0 values in their control and treatment arms are excluded because it makes calculating the summary effect impossible.

&nbsp;

After the exclusion process, our final data contained the data largest meta-analysis from a total of 1,415 reviews. 


## Exploritory Data Analysis

We first examine the distribution of the number of studies used in the largest meta analysis for each review (analyses with less than 5 studies were excluded during data cleaning):

```{r, cache=FALSE}

n.studies.hist

```

We see that most of the analyses contain between 5 and 10 studies. The median number of studies used in the meta-analysis is 9. The largest meta-analysis (in terms of number of studies) is CD004125. It compares drugs for preventing post operative nausea and contains 489 studies (not pictured). 

&nbsp;

We then examine the distribution of the meta sample size for each meta-analysis:

```{r, cache=FALSE}
meta.n.density.plot
```

The smallest meta sample is 93 and the largest is 3,231,551 (not pictured). The median sample size is 1,743. The largest meta-analysis (in terms of sample size) is CD000974. It examines the effictiveness of Cholera vaccines. 

&nbsp;

We also count the number of times each model was used for the analysis in the original review:

```{r, results = 'asis'}

testing = full %>% group_by(my_ID) %>% slice(1) %>% ungroup()
testing$analysis_model = as.factor(testing$analysis_model)
fix.rand.table = data.frame("Fixed Effect" = 1206, "Random Effects" = 579)

stargazer(fix.rand.table, header = FALSE,
          rownames = FALSE, table.placement = 'H',
          summary = FALSE,
          title = "Counts for Each Analysis Model ")

```

We see that even though the random effects model is recommended for systematic reviews, it is only used in 32% of the meta-analyses. 

## Analysis

For each of the 1,415 reviews, we conduct a random effects meta-analysis to estimate the summary effect. We elected to use odds-ratios as our summary measure. For each meta-analysis, we used each of the 7 between-study variance estimation methods and stored the overall summary estimate, the estimate for $\tau^2$, and the estimate for the standard error of $\tau^2$. 

&nbsp;

We first look at the differences in the $\tau^2$ estimates for 60 randomly selected meta-analyses:

```{r, fig.height=8, fig.width=12, cache = FALSE}
tau.estimates.plot
```




```{r, results = 'asis'}
stargazer(sums4, header = FALSE,
          summary = FALSE, table.placement = 'H')
```

```{r}
tau.square.density.plot
```

```{r}
tau.se.density.plot
```



```{r, fig.height=8, fig.width=12}
summary.estimates.plot
```


```{r}
p.diff.plot
```


```{r}

test = full %>%
  gather(key = "mod", value = "taus", DL.tau2, PM.tau2,
         HE.tau2, HS.tau2, ML.tau2, REML.tau2, SJ.tau2) %>%
  group_by(my_ID, mod) %>%
  slice(1) %>%
  ungroup()

test1 = test %>%
  group_by(my_ID) %>%
  mutate(differ = max(taus) - min(taus)) %>%
  slice(1)

test2 = lm(differ ~ log(meta.n), data = test1)
summary(test2)

```


